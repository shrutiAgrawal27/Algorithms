{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    " \n",
    "#sys.path.append('../utils')\n",
    "# from azure_blob_utils import AzureBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a74879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(pick_path: str, material_path: str, qoh_path: str,\n",
    "              res_mat_path: str, master_noun_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load pick data from Azure and material base from local CSV.\n",
    " \n",
    "    Args:\n",
    "        blob (AzureBlob): AzureBlob instance for reading from blob storage.\n",
    "        pick_path (str): Path to pick data CSV on blob.\n",
    "        material_path (str): Path to material master data.\n",
    "        qoh_path (str): Path to quantity on hand data.\n",
    "        res_mat_path (str): Path to material with reserved bin data.\n",
    "        master_noun_path (str): Path to validated master noun data.\n",
    " \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: Loaded pick data and material base.\n",
    "    \"\"\"\n",
    "    df_pick = pd.read_csv(pick_path)\n",
    "    #material_base = blob.read_excel(material_path, sheet_name=\"Material Base Data\")\n",
    "    df_qoh = pd.read_csv(qoh_path)\n",
    "    df_res_mat = pd.read_excel(res_mat_path)\n",
    "    df_material = pd.read_csv(material_path)\n",
    "    df_master_noun = pd.read_csv(master_noun_path)\n",
    "    df_material = df_material[df_material['XStatus'] == 'ZA']    # Filter material master data to get the active materials\n",
    "\n",
    "    # Step 1: Extract unique materials from qoh and reserved bin\n",
    "    materials_qoh = df_qoh['Material'].astype(str).str.strip().unique()\n",
    "    materials_res_mat = df_res_mat['Material Assigned'].astype(str).str.strip().unique()\n",
    "\n",
    "    # Step 2: Combine into a set to ensure uniqueness\n",
    "    material_set = set(materials_qoh).union(materials_res_mat)\n",
    "\n",
    "    # Step 3: Filter active materials to include only rows where 'Material' is in the material_set\n",
    "    material_base = df_material[df_material['Material'].astype(str).str.strip().isin(material_set)]\n",
    " \n",
    "    df_master_noun['Material'] = df_master_noun['Material'].astype(str).str.strip()\n",
    "    material_base['Material'] = material_base['Material'].astype(str).str.strip()\n",
    "    df_pick['MATNR'] = df_pick['MATNR'].astype(str).str.strip()\n",
    "\n",
    "    # Now merge\n",
    "    material_base = material_base.merge(df_master_noun[['Material', 'Master_Noun']], \n",
    "                              on='Material', \n",
    "                              how='left')\n",
    " \n",
    "    return df_pick, material_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df: pd.DataFrame, date_col: str = 'Dates', material_col: str = 'MATNR',\n",
    "                 to_col: str = 'TO Number') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the data by extracting monthly counts of material movements.\n",
    " \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing material, date, and transfer order number.\n",
    "        date_col (str): Name of the column containing date.\n",
    "        material_col (str): Name of the column containing material codes.\n",
    "        to_col (str): Name of the column containing transfer order numbers.\n",
    " \n",
    "    Returns:\n",
    "        pd.DataFrame: Pivoted DataFrame with months as index and materials as columns.\n",
    "    \"\"\"\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df['Month'] = df[date_col].dt.to_period('M')\n",
    "    grouped = df.groupby(['Month', material_col])[to_col].nunique().reset_index()\n",
    "    pivot = grouped.pivot(index='Month', columns=material_col, values=to_col).fillna(0)\n",
    "    pivot.index = pivot.index.to_timestamp()\n",
    "    return pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5081af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pick_data_by_date(df: pd.DataFrame, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter pick data using a date range with .between().\n",
    " \n",
    "    Args:\n",
    "        df (pd.DataFrame): Pick data with 'Dates' column.\n",
    "        start (str): Start date in 'YYYY-MM-DD'.\n",
    "        end (str): End date in 'YYYY-MM-DD'.\n",
    " \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered pick data.\n",
    "    \"\"\"\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'])\n",
    "    return df[df['Dates'].between(start, end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0853b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_seasonality_strength(ts, model='additive', period=12):\n",
    "    \"\"\"\n",
    "    Calculate the strength of seasonality (FS) for a time series.\n",
    " \n",
    "    FS = max(0, 1 - Var(R_t) / Var(S_t + R_t))\n",
    " \n",
    "    Args:\n",
    "        ts (pd.Series): Time series indexed by datetime.\n",
    "        model (str): Type of decomposition model - 'additive' or 'multiplicative'. Default is 'additive'.\n",
    "        period (int): Number of periods in a season (e.g., 12 for monthly seasonality).\n",
    " \n",
    "    Returns:\n",
    "        float: Seasonality strength (FS) in range [0, 1].\n",
    "    \"\"\"\n",
    "    result = seasonal_decompose(ts, model=model, period=period)\n",
    "    R_t = result.resid.dropna()\n",
    "    S_t = result.seasonal.loc[R_t.index]\n",
    "    FS = max(0, 1 - (np.var(R_t) / np.var(S_t + R_t)))\n",
    "    return FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_seasonal_months(ts, model='additive', period=12, top_n=3):\n",
    "    \"\"\"\n",
    "    Identifies top N seasonal months based on magnitude from seasonal decomposition.\n",
    "\n",
    "    Args:\n",
    "        ts (pd.Series): Time series for one material with datetime index.\n",
    "        model (str): Type of decomposition ('additive' or 'multiplicative').\n",
    "        period (int): Seasonality period (e.g., 12 for monthly).\n",
    "        top_n (int): Number of top seasonal months to return based on magnitude.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Fiscal month names with strongest seasonal impact.\n",
    "    \"\"\"\n",
    "    result = seasonal_decompose(ts, model=model, period=period)\n",
    "    seasonal = result.seasonal\n",
    " \n",
    "    # Get top N months by absolute seasonal component\n",
    "    monthly_avg = seasonal.groupby(seasonal.index.month).mean()\n",
    "    positive_peaks = monthly_avg[monthly_avg > 0]  # only positive seasonal values\n",
    "    top_months = positive_peaks.sort_values(ascending=False).head(top_n).index\n",
    " \n",
    " \n",
    "    #top_months = seasonal.abs().groupby(seasonal.index.month).mean().nlargest(top_n).index\n",
    " \n",
    "    # Map calendar months (1=Jan, ..., 12=Dec) to fiscal names (Apr=1, ..., Mar=12)\n",
    "    fiscal_month_names = ['April', 'May', 'June', 'July', 'August', 'September',\n",
    "                          'October', 'November', 'December', 'January', 'February', 'March']\n",
    "\n",
    "    # Convert calendar month to fiscal index: (month - 4) % 12\n",
    "    fiscal_indices = ((top_months - 4) % 12)\n",
    "    unique_fiscal_indices = sorted(fiscal_indices.tolist(), key=lambda x: (x + 12) % 12)\n",
    " \n",
    "    return [fiscal_month_names[i] for i in unique_fiscal_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_seasonality(pivot, period=12):\n",
    "    \"\"\"\n",
    "    Analyze seasonality strength and significant months for each material.\n",
    " \n",
    "    Args:\n",
    "        pivot (pd.DataFrame): Pivoted time series with 'Month' as index and materials as columns.\n",
    "        period (int): Seasonality period (e.g., 12 for monthly data).\n",
    " \n",
    "    Returns:\n",
    "        seasonality_strength_df (pd.DataFrame): FS scores and seasonality category.\n",
    "        seasonal_months_df (pd.DataFrame): Material and strong seasonal months.\n",
    "    \"\"\"\n",
    "    seasonality_scores = []\n",
    "    material_seasonal_months = []\n",
    " \n",
    "    for material in pivot.columns:\n",
    "        ts = pivot[material].dropna()\n",
    "        if len(ts) == 0:\n",
    "            continue\n",
    " \n",
    "        fs = calculate_seasonality_strength(ts, period=period)\n",
    " \n",
    "        # Assign category\n",
    "        if fs >= 0.7:\n",
    "            category = 'High'\n",
    "        elif fs >= 0.3:\n",
    "            category = 'Medium'\n",
    "        else:\n",
    "            category = 'Low'\n",
    " \n",
    "        seasonality_scores.append({'Material': material, 'FS': fs, 'Seasonality': category})\n",
    " \n",
    "        if fs >= 0.3:\n",
    "            months = get_significant_seasonal_months(ts, period=period)\n",
    "            for month in months:\n",
    "                material_seasonal_months.append({'Material': material, 'seasonal_month': month})\n",
    " \n",
    "    return pd.DataFrame(seasonality_scores), pd.DataFrame(material_seasonal_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c4b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_materials_by_season(months_df):\n",
    "    \"\"\"\n",
    "    Groups materials by their seasonal months and assigns a calendar-ordered group name.\n",
    " \n",
    "    Args:\n",
    "        months_df (pd.DataFrame): DataFrame with 'Material' and 'seasonal_month' columns.\n",
    " \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'Material' and 'seasonal_group' (calendar sorted string of months).\n",
    "    \"\"\"\n",
    "    calendar_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                      'July', 'August', 'September', 'October', 'November', 'December']\n",
    " \n",
    "    # Create a mapping from month to calendar index\n",
    "    month_order_map = {month: i for i, month in enumerate(calendar_order)}\n",
    " \n",
    "    return (\n",
    "        months_df.groupby('Material')['seasonal_month']\n",
    "        .apply(lambda x: ', '.join(sorted(x.unique(), key=lambda m: month_order_map[m])))\n",
    "        .reset_index(name='seasonal_group')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e053678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpivot_and_add_seasonal_group(pivot, material_group_df):\n",
    "    \"\"\"\n",
    "    Unpivots the pivot table and adds seasonal group info.\n",
    " \n",
    "    Args:\n",
    "        pivot (pd.DataFrame): Pivoted time series with 'Month' as index and materials as columns.\n",
    "        material_group_df (pd.DataFrame): DataFrame with 'Material' and 'seasonal_group' columns.\n",
    " \n",
    "    Returns:\n",
    "        pd.DataFrame: Unpivoted DataFrame with seasonal group.\n",
    "    \"\"\"\n",
    "    pivot_reset = pivot.reset_index()\n",
    "    df_unpivoted = pd.melt(pivot_reset, id_vars='Month', var_name='Material', value_name='TO_Count')\n",
    "\n",
    "    # Map seasonal group to materials\n",
    "    material_to_group = dict(zip(material_group_df['Material'], material_group_df['seasonal_group']))\n",
    "    df_unpivoted['seasonal_group'] = df_unpivoted['Material'].map(material_to_group)\n",
    " \n",
    "    return df_unpivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(df1: pd.DataFrame, df2: pd.DataFrame, df3: pd.DataFrame, prefix: str = \"seasonality_groups\"):\n",
    "    \"\"\"\n",
    "    Save Material Base with clusters, Monthly TOs, Seasonality Decomposition to a single Excel file with separate sheets.\n",
    " \n",
    "    Args:\n",
    "        df1, df2, df3 (pd.DataFrame): DataFrames to save.\n",
    "        file_path (str): Path to the output Excel file.\n",
    "    \"\"\"\n",
    " \n",
    "    timestamp = datetime.now().strftime('%Y%m%d')\n",
    "#     file_path = f\"{prefix}_{timestamp}.xlsx\"\n",
    " \n",
    "#     with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "#         df1.to_excel(writer, sheet_name='Material Base with clusters', index=False)\n",
    "#         df2.to_excel(writer, sheet_name='Monthly TOs', index=False)\n",
    "#         df3.to_excel(writer, sheet_name='Seasonality FS', index=False)\n",
    "    df1.to_csv(r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\Material Base with clusters', index=False)\n",
    "    df2.to_csv(r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\Monthly TOs', index=False)\n",
    "    df3.to_csv(r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\Seasonality FS', index=False)\n",
    " \n",
    "    print(f\"âœ… Data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4204371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_seasonality_threshold(seasonality_strength_df, percentile_method=False, domain_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Decide the optimal threshold for FS (Seasonality Strength).\n",
    "\n",
    "    Args:\n",
    "    - seasonality_strength_df (DataFrame): DataFrame containing 'FS' values for each material.\n",
    "    - percentile_method (bool): Whether to use percentile method to determine threshold (default False).\n",
    "    - domain_threshold (float): Initial threshold based on domain knowledge (default 0.3).\n",
    "\n",
    "    Returns:\n",
    "    - float: The chosen threshold for strong seasonality.\n",
    "    - DataFrame: Materials with FS > threshold.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Plot Distribution of FS\n",
    "    plt.hist(seasonality_strength_df['FS'], bins=30, edgecolor='black')\n",
    "    plt.title('Distribution of FS (Seasonality Strength)')\n",
    "    plt.xlabel('FS')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "    # Step 2: Domain Knowledge-based Threshold (Initial Guess)\n",
    "    print(f\"Initial Domain Knowledge Threshold: {domain_threshold}\")\n",
    "\n",
    "    # Step 3: Empirical Threshold (Percentile-based)\n",
    "    if percentile_method:\n",
    "        percentile_90 = np.percentile(seasonality_strength_df['FS'], 90)\n",
    "        percentile_80 = np.percentile(seasonality_strength_df['FS'], 80)\n",
    "        print(f\"90th Percentile Threshold: {percentile_90}\")\n",
    "        print(f\"80th Percentile Threshold: {percentile_80}\")\n",
    "\n",
    "        # Use the 90th percentile as threshold (can change based on needs)\n",
    "        chosen_threshold = percentile_90\n",
    "    else:\n",
    "        # Default to domain-based threshold if no percentile method is used\n",
    "        chosen_threshold = domain_threshold\n",
    " \n",
    "    # Step 4: Filter Materials Above Chosen Threshold\n",
    "    seasonal_materials = seasonality_strength_df[seasonality_strength_df['FS'] > chosen_threshold]\n",
    "\n",
    "    print(f\"\\nâœ… Materials with strong seasonality (FS > {chosen_threshold}):\")\n",
    "    print(seasonal_materials.sort_values(by='FS', ascending=False))\n",
    "\n",
    "    # Step 5: Return the chosen threshold and filtered materials\n",
    "    return chosen_threshold, seasonal_materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Full pipeline for seasonal clustering of materials for warehouse optimization.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize blob and paths\n",
    "#     blob = AzureBlob(storage_account_name=\"250501pdeus2nedcst01\")\n",
    "    pick_path = r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\4.1.2025 - Orders & Dispatch_Raw_Picking_Data.csv'\n",
    "    material_path = r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\4.1.2025 - All Material IDs w Long Description.csv'\n",
    "    qoh_path = r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\4.1.2025 - Inventory Management QOH.csv'\n",
    "    res_mat_path = r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\14.04.25 - storage_bins_reserved_material.xlsx'\n",
    "    master_noun_path = r'C:\\Users\\Shruti.Agarwal\\Desktop\\code for review\\Seasonality Analysis\\Validated_Master_Noun_List.csv'\n",
    " \n",
    " \n",
    "    # Load and filter\n",
    "    df_pick, material_base = load_data(pick_path, material_path, qoh_path, res_mat_path, master_noun_path)\n",
    "    df_filtered = filter_pick_data_by_date(df_pick, '2022-04-01', '2025-03-31')\n",
    " \n",
    "    # Prep subsets\n",
    "    df = df_filtered[df_filtered['MATNR'].isin(material_base['Material'])]\n",
    "    pivot = prepare_data(df, \"Dates\", \"MATNR\", \"TO Number\")\n",
    " \n",
    "    # Analyze Seasonality\n",
    "    seasonality_strength_df, seasonal_months_df = analyze_seasonality(pivot, period=12)\n",
    " \n",
    "    # Identify Thresholds\n",
    "    #chosen_threshold, seasonal_materials = decide_seasonality_threshold(seasonality_strength_df,\n",
    "    #                                        percentile_method=False, domain_threshold=0.3)\n",
    " \n",
    "    # Group Seasonal Months into Seasonal Group\n",
    "    material_group_df = group_materials_by_season(seasonal_months_df)\n",
    " \n",
    "    # Unpivot Data and Merge Seasonal Group\n",
    "    unpivoted_df = unpivot_and_add_seasonal_group(pivot, material_group_df)\n",
    " \n",
    "    # Merge Seasonality Category\n",
    "    unpivoted_df = unpivoted_df.merge(\n",
    "        seasonality_strength_df[['Material', 'Seasonality']], on='Material', how='left')\n",
    " \n",
    "    # ðŸ”¹ Export\n",
    "    final_df = material_base.merge(material_group_df[['Material', 'seasonal_group']], on='Material', how='left')\n",
    "    export_results(final_df, unpivoted_df, seasonality_strength_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272405a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
